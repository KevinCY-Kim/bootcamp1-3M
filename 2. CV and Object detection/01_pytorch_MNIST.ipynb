{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":209,"metadata":{"id":"BJ4LcAssuNVL","executionInfo":{"status":"ok","timestamp":1748496645729,"user_tz":-540,"elapsed":57,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","source":["# 배치 크기 × 채널 × 높이(height) × 너비(widht)의 크기의 텐서를 선언\n","inputs = torch.Tensor(1, 1, 28, 28)\n","print('텐서의 크기 : {}'.format(inputs.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j98PVl47uP7e","outputId":"324e3cd0-b123-419a-f234-3f78c75db8f3","executionInfo":{"status":"ok","timestamp":1748496646538,"user_tz":-540,"elapsed":28,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":210,"outputs":[{"output_type":"stream","name":"stdout","text":["텐서의 크기 : torch.Size([1, 1, 28, 28])\n"]}]},{"cell_type":"code","source":["conv1 = nn.Conv2d(1, 64, 3, padding=1) # 필터의 채널(=in_channel), 개수(out_channel), 크기\n","print(conv1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xaJG115buRjN","outputId":"ce8d2166-7e4e-4f04-f44a-f7785ac6aa1c","executionInfo":{"status":"ok","timestamp":1748496647834,"user_tz":-540,"elapsed":25,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":211,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"]}]},{"cell_type":"code","source":["pool = nn.MaxPool2d(2)\n","print(pool)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gslcjt4UUcD-","executionInfo":{"status":"ok","timestamp":1748496649694,"user_tz":-540,"elapsed":25,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"2159ffa7-3295-4abc-8658-ea16e4d22c88"},"execution_count":212,"outputs":[{"output_type":"stream","name":"stdout","text":["MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"]}]},{"cell_type":"code","source":["conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","print(conv2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHLj03TyuUGb","outputId":"b4c861c8-6d1e-4ce8-e013-3bdaf07b7e66","executionInfo":{"status":"ok","timestamp":1748496651125,"user_tz":-540,"elapsed":36,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":213,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"]}]},{"cell_type":"code","source":["pool = nn.MaxPool2d(2)\n","print(pool)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoFQFO1huWFQ","outputId":"cff282cd-7642-4da3-b91a-32a3574b1839","executionInfo":{"status":"ok","timestamp":1748496653216,"user_tz":-540,"elapsed":41,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":214,"outputs":[{"output_type":"stream","name":"stdout","text":["MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"]}]},{"cell_type":"code","source":["conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","print(conv3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpoV6lHxSPws","executionInfo":{"status":"ok","timestamp":1748496654465,"user_tz":-540,"elapsed":39,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"5cfc6deb-8eef-481a-881f-c2c44c684ac5"},"execution_count":215,"outputs":[{"output_type":"stream","name":"stdout","text":["Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"]}]},{"cell_type":"code","source":["pool = nn.MaxPool2d(2)\n","print(pool)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jk0tETo8SRKV","executionInfo":{"status":"ok","timestamp":1748496655892,"user_tz":-540,"elapsed":47,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"06764bc4-c7d8-406a-a6c3-aeab78dc2c71"},"execution_count":216,"outputs":[{"output_type":"stream","name":"stdout","text":["MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"]}]},{"cell_type":"markdown","source":["지금까지는 선언만한 것이고 아직 이들을 연결시키지는 않았습니다. 이들을 연결시켜서 모델을 완성시켜보겠습니다. 우선 입력을 첫번째 합성곱층을 통과시키고 합성곱층을 통과시킨 후의 텐서의 크기를 보겠습니다."],"metadata":{"id":"Ine1CWGwubhT"}},{"cell_type":"code","source":["out = conv1(inputs)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1slraBJzuXpk","outputId":"6d53f386-f147-4a50-b398-96c3e1e36346","executionInfo":{"status":"ok","timestamp":1748496660828,"user_tz":-540,"elapsed":49,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":217,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 64, 28, 28])\n"]}]},{"cell_type":"code","source":["out = pool(out)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBRE6gyFuZek","outputId":"4f35a32d-0e0c-4fd2-9fee-8196af22e9ec","executionInfo":{"status":"ok","timestamp":1748496662114,"user_tz":-540,"elapsed":63,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":218,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 64, 14, 14])\n"]}]},{"cell_type":"code","source":["out = conv2(out)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvyLe9Ewuduo","outputId":"e96f92d1-d076-4fff-f08c-b69271f293e5","executionInfo":{"status":"ok","timestamp":1748496663429,"user_tz":-540,"elapsed":60,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":219,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 128, 14, 14])\n"]}]},{"cell_type":"code","source":["out = pool(out)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"179tAmEyUnTT","executionInfo":{"status":"ok","timestamp":1748496665472,"user_tz":-540,"elapsed":70,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"c5e10765-c928-41ae-f1f7-a74f3a9ac091"},"execution_count":220,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 128, 7, 7])\n"]}]},{"cell_type":"code","source":["out = conv3(out)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxGRyTJGSmRL","executionInfo":{"status":"ok","timestamp":1748496667357,"user_tz":-540,"elapsed":64,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"dc617067-4558-4c1d-e38c-d56281e7e5da"},"execution_count":221,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 256, 7, 7])\n"]}]},{"cell_type":"code","source":["out = pool(out)\n","print(out.shape) # batch, c, h, w -> batch, cxhxw"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TsT8SdmfufB2","outputId":"6bd9f005-4183-4537-d5d6-5964191fdb54","executionInfo":{"status":"ok","timestamp":1748496669491,"user_tz":-540,"elapsed":86,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":222,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 256, 3, 3])\n"]}]},{"cell_type":"code","source":["# 첫번째 차원인 배치 차원은 그대로 두고 나머지는 펼쳐라\n","out = out.view(out.size(0), -1) # (0)은 out\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7IpPmZfugKK","outputId":"579ada31-7099-4440-8e39-829e5aa24a65","executionInfo":{"status":"ok","timestamp":1748496671134,"user_tz":-540,"elapsed":83,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":223,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2304])\n"]}]},{"cell_type":"code","source":["fc = nn.Linear(2304, 10) # input_dim = 3,136, output_dim = 10\n","out = fc(out)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0RqOcWwunIN","outputId":"62a348c5-a6bf-46e5-bdf2-5441ff53f1d1","executionInfo":{"status":"ok","timestamp":1748496678726,"user_tz":-540,"elapsed":86,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":224,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10])\n"]}]},{"cell_type":"markdown","source":["## CNN으로 MNIST 분류하기"],"metadata":{"id":"pVEh08xIuofR"}},{"cell_type":"code","source":["import torch\n","import torchvision.datasets as dsets\n","import torchvision.transforms as transforms\n","import torch.nn.init"],"metadata":{"id":"_h1sZPsfuspH","executionInfo":{"status":"ok","timestamp":1748496680867,"user_tz":-540,"elapsed":105,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":225,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# 랜덤 시드 고정 : 고정된 무작위성\n","torch.manual_seed(777)\n","\n","# GPU 사용 가능일 경우 랜덤 시드 고정\n","if device == 'cuda':\n","    torch.cuda.manual_seed_all(777)"],"metadata":{"id":"dy2_fiTDutva","executionInfo":{"status":"ok","timestamp":1748496682735,"user_tz":-540,"elapsed":107,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":226,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.001\n","training_epochs = 15\n","batch_size = 256"],"metadata":{"id":"u-5mOAlQuvvz","executionInfo":{"status":"ok","timestamp":1748496684473,"user_tz":-540,"elapsed":118,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":227,"outputs":[]},{"cell_type":"code","source":["mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n","                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n","                          transform=transforms.ToTensor(), # 텐서로 변환\n","                          download=True)\n","\n","mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n","                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n","                         transform=transforms.ToTensor(), # 텐서로 변환\n","                         download=True)"],"metadata":{"id":"ZuptDOgduw-r","executionInfo":{"status":"ok","timestamp":1748496686673,"user_tz":-540,"elapsed":137,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":228,"outputs":[]},{"cell_type":"code","source":["data_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n","                                          batch_size=batch_size,\n","                                          shuffle=True,\n","                                          drop_last=True)"],"metadata":{"id":"EKjOVxDyuyc0","executionInfo":{"status":"ok","timestamp":1748496688580,"user_tz":-540,"elapsed":136,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":229,"outputs":[]},{"cell_type":"code","source":["class CNN(torch.nn.Module):\n","\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        # 첫번째층\n","        # ImgIn shape=(?, 1, 28, 28)\n","        #    Conv     -> (?, 32, 28, 28)\n","        #    Pool     -> (?, 32, 14, 14)\n","        self.layer1 = torch.nn.Sequential(\n","            torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        # 두번째층\n","        # ImgIn shape=(?, 32, 14, 14)\n","        #    Conv      ->(?, 64, 14, 14)\n","        #    Pool      ->(?, 64, 7, 7)\n","        self.layer2 = torch.nn.Sequential(\n","            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.layer3 = torch.nn.Sequential(\n","            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","        # 전결합층 7x7x64 inputs -> 10 outputs\n","\n","        self.fc = torch.nn.Linear(3 * 3 * 256, 10, bias=True)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n","        out = self.fc(out)\n","        return out"],"metadata":{"id":"9OjnULsIu1dr","executionInfo":{"status":"ok","timestamp":1748496714207,"user_tz":-540,"elapsed":157,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":230,"outputs":[]},{"cell_type":"code","source":["# CNN 모델 정의\n","model = CNN().to(device) # .to('cuda') == .cuda() / .to('cpu') == .cpu()"],"metadata":{"id":"h39wXiBPu4Q7","executionInfo":{"status":"ok","timestamp":1748496716625,"user_tz":-540,"elapsed":156,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":231,"outputs":[]},{"cell_type":"markdown","source":["Loss\n","- pytorch에서는 CrossEntropyLoss()로 sparse categorical, categorical 두개를 자동으로 인식하여 처리한다. (softmax 포함하기 때문에 모델에서 Softmax 생략)\n","- 이진분류 경우에는 BCELoss() 사용 (sigmoid 미포함이므로 모델에서 sigmoid 선언)"],"metadata":{"id":"RDejqCN7jQBq"}},{"cell_type":"code","source":["criterion = torch.nn.CrossEntropyLoss()   # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"YJwvKLdcu5g-","executionInfo":{"status":"ok","timestamp":1748496718779,"user_tz":-540,"elapsed":196,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":232,"outputs":[]},{"cell_type":"code","source":["total_batch = len(data_loader)\n","print('총 배치의 수 : {}'.format(total_batch))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lea4TRDBu7rT","outputId":"618f2828-0219-431f-9eca-e896990a8aed","executionInfo":{"status":"ok","timestamp":1748496720744,"user_tz":-540,"elapsed":187,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":233,"outputs":[{"output_type":"stream","name":"stdout","text":["총 배치의 수 : 39\n"]}]},{"cell_type":"code","source":["for epoch in range(training_epochs):\n","    avg_cost = 0\n","\n","    for X, Y in data_loader:\n","        X = X.to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X) # forward 실행\n","        cost = criterion(hypothesis, Y) # CrossEntropyLoss\n","        cost.backward() # gradient 계산\n","        optimizer.step() # weight 업데이트\n","\n","        avg_cost += cost / total_batch\n","\n","    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JUo3rX1ku85T","outputId":"34f3e8a1-276f-4234-ce08-8c9fc34003ad","executionInfo":{"status":"ok","timestamp":1748496739314,"user_tz":-540,"elapsed":16033,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":234,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch:    1] cost = 0.876085997\n","[Epoch:    2] cost = 0.167580098\n","[Epoch:    3] cost = 0.0987980291\n","[Epoch:    4] cost = 0.0620296896\n","[Epoch:    5] cost = 0.0434814915\n","[Epoch:    6] cost = 0.0338782892\n","[Epoch:    7] cost = 0.0240439177\n","[Epoch:    8] cost = 0.0139407618\n","[Epoch:    9] cost = 0.00962143578\n","[Epoch:   10] cost = 0.00796762668\n","[Epoch:   11] cost = 0.00497771287\n","[Epoch:   12] cost = 0.00534754712\n","[Epoch:   13] cost = 0.0045820158\n","[Epoch:   14] cost = 0.00224256539\n","[Epoch:   15] cost = 0.00161633454\n"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","    X_test = mnist_train.train_data.view(len(mnist_train), 1, 28, 28).float().to(device)\n","    Y_test = mnist_train.train_labels.to(device)\n","\n","    prediction = model(X_test)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_test\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"NFcOYB4Iu_Gf","outputId":"ae4e2f6c-0e72-4d61-a787-99af2636ab5c","executionInfo":{"status":"error","timestamp":1748496745120,"user_tz":-540,"elapsed":286,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":236,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 11.22 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.36 GiB is free. Process 5220 has 12.38 GiB memory in use. Of the allocated memory 11.59 GiB is allocated by PyTorch, and 664.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-236-521709c1609b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mcorrect_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-230-e8c6b16ccfd6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 11.22 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.36 GiB is free. Process 5220 has 12.38 GiB memory in use. Of the allocated memory 11.59 GiB is allocated by PyTorch, and 664.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["import gc\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_7BDecfgadb","executionInfo":{"status":"ok","timestamp":1748496757273,"user_tz":-540,"elapsed":72,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"d76a9d53-b202-494c-ee1b-35094f74faf2"},"execution_count":243,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":243}]},{"cell_type":"code","source":["with torch.no_grad():\n","    X_train = mnist_train.train_data[:1000].view(1000, 1, 28, 28).float().to(device)\n","    Y_train = mnist_train.train_labels[:1000].to(device)\n","\n","    prediction = model(X_train)\n","    correct_prediction = torch.argmax(prediction, 1) == Y_train\n","    accuracy = correct_prediction.float().mean()\n","    print('Accuracy:', accuracy.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXZcHsWNgCFF","executionInfo":{"status":"ok","timestamp":1748496813823,"user_tz":-540,"elapsed":29,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"9464c81e-71c6-4a29-cb52-b26a08b46c56"},"execution_count":245,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9750000238418579\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dBwXRvM6f9Hk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 깊은 CNN으로 MNIST 분류하기"],"metadata":{"id":"I73AbLS0vIAj"}},{"cell_type":"markdown","source":["직접 해봅시다.\n","\n","위 코드를 참고하여, 더 깊은 CNN 레이어를 쌓아보고, 학습시켜봅시다."],"metadata":{"id":"ziHIm9tZvkPE"}},{"cell_type":"markdown","source":["##### 정답"],"metadata":{"id":"4ETMZyC4dxef"}},{"cell_type":"code","source":["\n","class CNN(torch.nn.Module):\n","\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.layer1 = torch.nn.Sequential(\n","            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.layer2 = torch.nn.Sequential(\n","            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.layer3 = torch.nn.Sequential(\n","            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.layer4 = torch.nn.Sequential(\n","            torch.nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n","\n","        # 전결합층 한정으로 가중치 초기화\n","        torch.nn.init.xavier_uniform_(self.fc.weight)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc(out)\n","        return out"],"metadata":{"id":"CAKC43Ed9hzA","executionInfo":{"status":"ok","timestamp":1748492123470,"user_tz":-540,"elapsed":6,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["model = CNN().cuda() # to('cuda')\n","\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"O9O0kq5mskzz","executionInfo":{"status":"ok","timestamp":1748492126077,"user_tz":-540,"elapsed":9,"user":{"displayName":"김창용","userId":"08165353884628186490"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["for epoch in range(training_epochs):\n","    for X, Y in data_loader:\n","        X = X.to(device)\n","        Y = Y.to(device)\n","\n","        optimizer.zero_grad()\n","        hypothesis = model(X)\n","        cost = criterion(hypothesis, Y)\n","        cost.backward()\n","        optimizer.step()"],"metadata":{"id":"hLnnhrbss2mT","colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"status":"error","timestamp":1748492128116,"user_tz":-540,"elapsed":143,"user":{"displayName":"김창용","userId":"08165353884628186490"}},"outputId":"252ee591-fc47-4cb3-f32c-0c5835e5d100"},"execution_count":25,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (256x256 and 3136x10)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-4e442972384c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-23-4309fbdd9ed1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x256 and 3136x10)"]}]}]}